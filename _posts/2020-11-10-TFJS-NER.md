---
title: 'Named entity recognition with simple Attention'
date: 2020-11-10
permalink: /posts/2020/11/TFJS-NER/
excerpt_separator: <!--more-->
toc: true
tags:
  - Tensorflow-JS
  - NLP
  - RNN
  - Text Classification
  - Keras
---

NER implementation hosted within browser using Tensorflow-JS.

 Definition from [Wikipedia](https://en.wikipedia.org/wiki/Named-entity_recognition) 
 > Named Entity Recognition is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, etc.
See [<b>*demo*</b>](/posts/2020/11/TFJS-NER/#demo) below. Continue reading for model explanation and code. 

<!--more-->
### Demo



   <head>
      <meta name="description" content="Testing Simple Machine Learning Model into an WebApp using TensorFlow.js">
      <meta name="keywords" content="Machine Learning, TensorFlow.js">
      <meta name="author" content="Mohit Pandey">
      <meta charset="UTF-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <meta http-equiv="X-UA-Compatible" content="ie=edge">
      <meta name="description" content="TensorFlow js demo for Named-entity recognition (NER) (Sequence Tagging task). Implemented with Keras (GloVe + GRU RNN) and tensorflow.js">
      <meta property="og:title" content="Named-entity recognition TensorFlow.js demo">
      <meta property="og:description" content="TensorFlow js demo for Named-entity recognition (NER) (Sequence Tagging task). Implemented with Keras (GloVe + GRU RNN) and tensorflow.js">
      <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
      <style>
         .demo {
         margin: 2em auto;
         }
         .main-result {
         margin: 3em auto;
         }
         .result {
         padding: 1em;
         }
         .demo-header {
         font-size: 1.0rem;
         margin: 0.5em;
         }
         .tags-review {
         margin-top: 1.5rem;
         }
         .divider{
          width:5px;
          height:auto;
          display:inline-block;
          }
         .btn-primary { background-color: red; }

      </style>
      
   </head>
   <body>
      <!--  <script src="https://code.jquery.com/jquery-2.2.4.min.js"></script>
         <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.5.2/dist/tf.min.js"></script> -->
      <main role="main" class="container">
         <p>
            Enter sentence like <code>Fischler proposed EU-wide measures after reports from Britain and France that under laboratory conditions sheep could contract bovine spongiform encephalopathy.</code>
            or <code>She likes David!</code>.
         </p>
         <div class="card demo">
            <div class="card-header">
               <!-- <h1 class="demo-header">
                  Dehcmcmo! -->
                  <!-- <div class="loading-model spinner-border text-primary" role="status">
                     <span class="sr-only">Loading...</span>
                  </div> -->
               <!-- </h1> -->
               <form class="form" onkeypress="return event.keyCode != 13;">
                  <div class="form-group mx-sm-3 md-2">
                     <input type="text" class="form-control form-control-xs" id='input_text' placeholder="Enter short sentence">
                  </div>
                  <div class="d-flex justify-content-center">
                     <button type="button" class="btn btn-primary" id="get_ner_button">Search Entities</button>
                     &nbsp;&nbsp;&nbsp;&nbsp;
                     <button type="button" class="btn btn-primary" id="clear_bttn">Clear</button>
                  </div>
               </form>
            </div>
            <div class="result main-result"></div>
            <div class="result attention-bar" id='attention_bar'></div>
            <div class="result tags-result"></div>
         </div>
      </main>
      <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
      <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
      <!-- <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@2.0.1/dist/tf.min.js"></script> -->
      <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"> </script>
      <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
      <script src="../../../../files/model/tfjs-ner/vocabs.js"></script>
      <script src="../../../../files/model/tfjs-ner/predict.js"></script>
      
   </body>

### Model Architecture 
#### Encoder-Attention-Decoder

##### Encoder

**Motivation**

Encode the source/imput sequence into a meaningful representation contatining information about the input sentence. In case of a problem involving images, this could be penultimate dense layer of a convolutional neural network. We'll see handling textual input in more detail.

**Implementation details with mathematical explanation and code**

Recurrent Neural Networks (RNN) are extremely suited to model sequential or temporal data. Text fits this category well. When a human reads a sentence, they process the current word they are reading $x_t$ while remembering what they had processed until then ${h_{t-1}}$. We model this using an RNN as follows

$$ h_t = RNN(h_{t-1},x_{t}) $$

where, $h_t$ is the hidden state of RNN (GRU/LSTM) at $t^{th}$ timestep. ${x_t}$ is a vectorial representation of the word (eg Word vector, Bag of Words etc.). \
For t = T, $h_T$ becomes our thought vector. In absence of an attention network, this thought vector is the input to decoder at decoder's timestep 0. i.e.

$$ s_0 = h_T $$

we'll talk more about $s_t$ in [decoder](#decoder) section.

Implementing encoder is a 2 step process.

If each word in the sentence is converted to a *d* dimension word vector, $x_i \in R^d$. Every sentence is normalized to same length T, typically equal to the longest sentence in the corpus. This is done by padding <\PAD> token to shorter sentences. Consequently, each sentence $Sent_i$ with T words becomes $Sent_i \in R^{T \times d} $. Following picture helps me visualize this
<img src="../../../../files/images/sent_dim.png"
     style=" margin-right: 5px;" width="70%" height="70%"/>
This is easy to translate to code
{% raw %}
```python
#MAX_SEQUENCE_LENGTH = T
words_input = Input(dtype='int32', shape=[MAX_SEQUENCE_LENGTH])
#words_input = [1,T] :1 = batchsize (for simplifying explanation)
#EMBEDDING_DIM = d (200 for used pretrained word2vec)
#embedding_tensor = weights from pretrained embedding. Dim: |Vocab| x d
x = Embedding(words_vocab_size + 1,
                EMBEDDING_DIM,
                weights=[embedding_tensor],
                input_length=MAX_SEQUENCE_LENGTH,
                trainable=False)(words_input)
#x = vectorized sentence (Sent_i):[1,T,d] :1 = batchsize (for simplifying explanation)
```
{% endraw %}
2. If there are $h$ units in each RNN (LSTM/GRU) block, each hidden RNN unit will produce an $h$ dimensional output called *hidden state*. (*Additionally we also get another output called cell state if using an LSTM. I'll write another blog-post detailing workings of LSTMs and GRUs.*)
Each $h_t \in R^h$. Note there are always T hidden units, one corresponding to each word. This would mean that output of the RNN block (RNN block comprises of all the $T$ hidden units together), will be in $R^{T \times h}$
{% raw %}
```python
#num_hidden_units = h
outputs = GRU(num_hidden_units,
                return_sequences=True,
                dropout=0.5,
                name='RNN_Layer')(x)
#outputs = [1,T,h] :1 = batchsize
```
{% endraw %}
In absence of the attention module, we only care for RNN output from last timestep $T$ as that ($h_T$) will be our *thought vector* (input to the decoder). For this, set <code>return_sequences = False</code> 

##### Attention

**Motivation**

The basic attention network courtesy [Bahdanau et.al](https://arxiv.org/abs/1409.0473) was originally proposed to solve and maximize the machine translation performance. However, it has been shown to perform exceedingly well in a wide variety of other downstream tasks as well such as NER, question answering, image classification etc. 

>Use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. 

A very primitive intituion to this idea of attention is to model human behavior towards sequence processing. Consider the task of question answering. The objective is to answer questions based upon understanding of a document (paragraph). The encoder encodes the entire sentence into one fixed dimension vector $h_T, (thought vector)$. It's unreasonable to expect this fixed vector to be equally effective in encoding the information from early timesteps $t \lt\lt T$ just as well as it would at $t\\approx T$. RNN's are prone to vanishing gradients hence making such a learning even harder. 
<br>A human on the other hand would not typically read the entire document (input) in order to make inferences. Humans pay selective focus (attention) to different parts of the sentence guided by the objective of the downstream task. So to answer a question about authorship of a document, a human reader will focus primarily on beginning of the document. An attention network tries to emulate this by learning to attend to different parts of the sentence with varying intensity (energy). 

The ability of attention network to assign higher scores to important phrases in texts and patches in images makes for interesting visualization and provides an interesting way for model explanation. In the case of Named Entitiy Recognition, the hope is that an attention module will learn to attend to most significant words, phrases and tokens that guide the classification of each word into classes (PER, MISC, LOC, ORG). In essence, we should see high attention score for words that belong to one of the named entity. 

**Implementation details with mathematical explanation and code**

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Cras tincidunt lobortis feugiat vivamus at augue eget arcu. Diam sollicitudin tempor id eu nisl nunc mi. Pulvinar proin gravida hendrerit lectus. Volutpat est velit egestas dui id ornare arcu. Nec feugiat in fermentum posuere urna nec tincidunt. Accumsan tortor posuere ac ut. Nam at lectus urna duis convallis convallis tellus. Metus aliquam eleifend mi in nulla. Facilisis gravida neque convallis a cras semper auctor neque. Aliquet sagittis id consectetur purus ut faucibus pulvinar elementum. Pellentesque massa placerat duis ultricies lacus sed turpis tincidunt id. Est lorem ipsum dolor sit amet consectetur adipiscing elit.

##### Decoder

### Code
{% raw %}
```python
from tensorflow.keras.callbacks import ModelCheckpoint
checkpoint = ModelCheckpoint('./trained_models/model.h5', monitor='loss', verbose=1, save_best_only=True, mode='min')
callbacks_list = [checkpoint]

model.fit(train_X, train_y,
          epochs=20,
          batch_size=64,
          validation_data=(valid_X, valid_y),
         callbacks=callbacks_list)
```
{% endraw %}
